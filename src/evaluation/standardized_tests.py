"""
æ ‡å‡†åŒ–æµ‹è¯•æ¡†æ¶ - ä½¿ç”¨ç›¸åŒæµ‹è¯•æŸ¥è¯¢å¯¹æ¯”ä¸‰ç§è®¾è®¡æ¨¡å¼
ç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼Œæ§åˆ¶å˜é‡ä¸€è‡´æ€§
"""

import time
import json
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

# æ ‡å‡†åŒ–æµ‹è¯•æŸ¥è¯¢é›†åˆ - æŒ‰å¤æ‚åº¦åˆ†ç±»
STANDARDIZED_TEST_QUERIES = {
    "simple": [
        "What's today's date?",
        "Hello, how are you?",
        "What is 2+2?",
        "Tell me about Python programming language",
        "What's the weather like?",
    ],
    "medium": [
        "Compare Python and JavaScript for web development",
        "Explain the concept of machine learning and give examples",
        "Create a simple todo list application design",
        "What are the benefits and drawbacks of remote work?",
        "How does blockchain technology work?",
    ],
    "complex": [
        "Design a scalable microservices architecture for an e-commerce platform with authentication, payments, and real-time notifications",
        "Create a comprehensive business plan for a sustainable energy startup including market analysis, financial projections, and risk assessment",
        "Develop a machine learning model deployment strategy considering data privacy, model versioning, and continuous monitoring",
        "Plan a complete digital transformation strategy for a traditional manufacturing company",
        "Design a disaster recovery plan for a multinational corporation's IT infrastructure",
    ],
}


@dataclass
class TestResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""

    pattern: str
    query: str
    complexity: str
    start_time: float
    end_time: float
    latency: float
    response_length: int
    success: bool
    error_message: Optional[str] = None
    steps_count: int = 0
    decision_points: int = 0
    tool_calls: int = 0


@dataclass
class PatternPerformance:
    """æ¨¡å¼æ€§èƒ½ç»Ÿè®¡"""

    pattern: str
    total_tests: int
    success_rate: float
    avg_latency: float
    min_latency: float
    max_latency: float
    avg_response_length: float
    total_steps: int
    total_decisions: int
    total_tool_calls: int

    def score(self) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ† - é‡æ–°è°ƒæ•´æƒé‡ï¼Œæ›´é‡è§†è´¨é‡"""
        # æˆåŠŸç‡æƒé‡35% (å¿…é¡»èƒ½å®Œæˆä»»åŠ¡)
        success_score = self.success_rate * 35

        # å“åº”è´¨é‡æƒé‡35% (è´¨é‡æœ€é‡è¦!)
        # ä½¿ç”¨æ›´åˆç†çš„è´¨é‡è¯„åˆ†ï¼šåŸºäºå“åº”é•¿åº¦ï¼Œä½†è®¾ç½®åˆç†åŒºé—´
        if self.avg_response_length < 500:
            quality_score = (
                self.avg_response_length / 500
            ) * 20  # 500å­—ç¬¦ä»¥ä¸‹æŒ‰æ¯”ä¾‹ç»™åˆ†
        elif self.avg_response_length < 2000:
            quality_score = (
                20 + ((self.avg_response_length - 500) / 1500) * 10
            )  # 500-2000å­—ç¬¦ï¼Œ20-30åˆ†
        else:
            quality_score = 30 + min(
                5, (self.avg_response_length - 2000) / 1000
            )  # 2000å­—ç¬¦ä»¥ä¸Šï¼Œæœ€é«˜35åˆ†

        # å»¶è¿Ÿè¯„åˆ†æƒé‡20% (é™ä½å»¶è¿Ÿæƒé‡)
        # ä½¿ç”¨æ›´å®½æ¾çš„å»¶è¿ŸåŸºå‡†ï¼š15ç§’ä¸ºåŸºå‡†
        if self.avg_latency <= 5:
            latency_score = 20  # 5ç§’ä»¥å†…æ»¡åˆ†
        elif self.avg_latency <= 15:
            latency_score = 20 - ((self.avg_latency - 5) / 10) * 15  # 5-15ç§’çº¿æ€§é€’å‡
        else:
            latency_score = max(
                0, 5 - (self.avg_latency - 15) * 0.5
            )  # 15ç§’ä»¥ä¸Šå¿«é€Ÿé€’å‡

        # åŠŸèƒ½å®Œæ•´æ€§æƒé‡10% (å·¥å…·ä½¿ç”¨ã€æ­¥éª¤å®Œæ•´æ€§)
        functionality_score = min(10, (self.total_tool_calls + self.total_steps) * 0.5)

        total_score = (
            success_score + quality_score + latency_score + functionality_score
        )

        return round(total_score, 2)


class StandardizedTester:
    """æ ‡å‡†åŒ–æµ‹è¯•å™¨"""

    def __init__(self):
        self.results: List[TestResult] = []

    async def test_pattern(
        self, pattern_name: str, pattern_graph, query: str, complexity: str
    ) -> TestResult:
        """æµ‹è¯•å•ä¸ªæ¨¡å¼"""
        start_time = time.time()
        success = True
        error_message = None
        response_length = 0
        steps_count = 0
        decision_points = 0
        tool_calls = 0

        try:
            # æ‰§è¡Œæµ‹è¯•
            response = pattern_graph.invoke(
                {"messages": [{"role": "user", "content": query}]}
            )

            end_time = time.time()

            # åˆ†æå“åº”
            if "messages" in response:
                last_message = response["messages"][-1]
                response_length = (
                    len(str(last_message.content))
                    if hasattr(last_message, "content")
                    else len(str(last_message))
                )

                # è®¡ç®—å·¥å…·è°ƒç”¨æ¬¡æ•°
                for msg in response["messages"]:
                    if hasattr(msg, "tool_calls") and msg.tool_calls:
                        tool_calls += len(msg.tool_calls)

            # åˆ†æç‰¹å®šæ¨¡å¼çš„æŒ‡æ ‡
            if pattern_name == "Sequential":
                # ç»Ÿè®¡é˜¶æ®µæ•°
                stage_markers = ["Planning Stage", "Execution Stage", "Review Stage"]
                for msg in response.get("messages", []):
                    content = str(msg.content if hasattr(msg, "content") else msg)
                    steps_count += sum(
                        1 for marker in stage_markers if marker in content
                    )

            elif pattern_name == "State-based":
                # ç»Ÿè®¡å†³ç­–ç‚¹
                if "decision_history" in response:
                    decision_points = len(response["decision_history"])
                else:
                    # é€šè¿‡å“åº”å†…å®¹ä¼°ç®—å†³ç­–ç‚¹
                    decision_markers = ["Analysis:", "Processing:", "Validation:"]
                    for msg in response.get("messages", []):
                        content = str(msg.content if hasattr(msg, "content") else msg)
                        decision_points += sum(
                            1 for marker in decision_markers if marker in content
                        )

        except Exception as e:
            end_time = time.time()
            success = False
            error_message = str(e)

        return TestResult(
            pattern=pattern_name,
            query=query,
            complexity=complexity,
            start_time=start_time,
            end_time=end_time,
            latency=end_time - start_time,
            response_length=response_length,
            success=success,
            error_message=error_message,
            steps_count=steps_count,
            decision_points=decision_points,
            tool_calls=tool_calls,
        )

    async def run_comprehensive_test(
        self, patterns: Dict[str, Any], queries_per_complexity: int = 2
    ) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„å¯¹æ¯”æµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹æ ‡å‡†åŒ–æ€§èƒ½æµ‹è¯•...")
        print("=" * 50)

        # æ¸…ç©ºä¹‹å‰çš„ç»“æœ
        self.results = []

        # å¯¹æ¯ä¸ªå¤æ‚åº¦çº§åˆ«è¿›è¡Œæµ‹è¯•
        for complexity, queries in STANDARDIZED_TEST_QUERIES.items():
            print(f"\nğŸ“Š æµ‹è¯•å¤æ‚åº¦: {complexity.upper()}")

            # é™åˆ¶æ¯ä¸ªå¤æ‚åº¦çš„æŸ¥è¯¢æ•°é‡
            test_queries = queries[:queries_per_complexity]

            for query in test_queries:
                print(f"\nğŸ“ æŸ¥è¯¢: {query[:50]}{'...' if len(query) > 50 else ''}")

                # å¯¹æ¯ä¸ªæ¨¡å¼æµ‹è¯•ç›¸åŒçš„æŸ¥è¯¢
                for pattern_name, pattern_graph in patterns.items():
                    print(f"   ğŸ”„ æµ‹è¯• {pattern_name}...", end=" ")

                    try:
                        result = await self.test_pattern(
                            pattern_name, pattern_graph, query, complexity
                        )
                        self.results.append(result)

                        if result.success:
                            print(f"âœ… {result.latency:.2f}s")
                        else:
                            print(f"âŒ å¤±è´¥: {result.error_message}")

                    except Exception as e:
                        print(f"âŒ é”™è¯¯: {e}")

        return self.generate_comparison_report()

    def generate_comparison_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        if not self.results:
            return {"error": "No test results available"}

        # æŒ‰æ¨¡å¼åˆ†ç»„ç»Ÿè®¡
        pattern_stats = {}

        for pattern in set(result.pattern for result in self.results):
            pattern_results = [r for r in self.results if r.pattern == pattern]

            if pattern_results:
                successful_results = [r for r in pattern_results if r.success]

                pattern_stats[pattern] = PatternPerformance(
                    pattern=pattern,
                    total_tests=len(pattern_results),
                    success_rate=len(successful_results) / len(pattern_results),
                    avg_latency=(
                        sum(r.latency for r in successful_results)
                        / len(successful_results)
                        if successful_results
                        else 0
                    ),
                    min_latency=(
                        min(r.latency for r in successful_results)
                        if successful_results
                        else 0
                    ),
                    max_latency=(
                        max(r.latency for r in successful_results)
                        if successful_results
                        else 0
                    ),
                    avg_response_length=(
                        sum(r.response_length for r in successful_results)
                        / len(successful_results)
                        if successful_results
                        else 0
                    ),
                    total_steps=sum(r.steps_count for r in pattern_results),
                    total_decisions=sum(r.decision_points for r in pattern_results),
                    total_tool_calls=sum(r.tool_calls for r in pattern_results),
                )

        # ç”Ÿæˆæ’å
        ranking = sorted(
            pattern_stats.items(), key=lambda x: x[1].score(), reverse=True
        )

        # æŒ‰å¤æ‚åº¦åˆ†æ
        complexity_analysis = {}
        for complexity in ["simple", "medium", "complex"]:
            complexity_results = [r for r in self.results if r.complexity == complexity]
            if complexity_results:
                complexity_analysis[complexity] = {
                    "avg_latency_by_pattern": {
                        pattern: sum(
                            r.latency
                            for r in complexity_results
                            if r.pattern == pattern and r.success
                        )
                        / len(
                            [
                                r
                                for r in complexity_results
                                if r.pattern == pattern and r.success
                            ]
                        )
                        for pattern in set(r.pattern for r in complexity_results)
                        if len(
                            [
                                r
                                for r in complexity_results
                                if r.pattern == pattern and r.success
                            ]
                        )
                        > 0
                    }
                }

        return {
            "timestamp": datetime.now().isoformat(),
            "total_tests": len(self.results),
            "pattern_performance": {
                name: asdict(stats) for name, stats in pattern_stats.items()
            },
            "ranking": [
                {"pattern": name, "score": stats.score()} for name, stats in ranking
            ],
            "complexity_analysis": complexity_analysis,
            "raw_results": [asdict(result) for result in self.results],
            "summary": self._generate_summary(pattern_stats, ranking),
        }

    def _generate_summary(
        self, pattern_stats: Dict[str, PatternPerformance], ranking: List
    ) -> Dict[str, str]:
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
        if not ranking:
            return {"error": "No ranking data available"}

        best_pattern = ranking[0][0]

        return {
            "overall_winner": f"{best_pattern} (ç»¼åˆè¯„åˆ†: {ranking[0][1]}åˆ†)",
            "fastest_pattern": min(
                pattern_stats.items(), key=lambda x: x[1].avg_latency
            )[0],
            "most_reliable": max(
                pattern_stats.items(), key=lambda x: x[1].success_rate
            )[0],
            "most_detailed": max(
                pattern_stats.items(), key=lambda x: x[1].avg_response_length
            )[0],
            "recommendations": {
                "simple_tasks": "ReAct - é€‚åˆå¿«é€Ÿå“åº”çš„ç®€å•æŸ¥è¯¢",
                "medium_tasks": "Sequential - é€‚åˆéœ€è¦ç³»ç»ŸåŒ–å¤„ç†çš„ä¸­ç­‰å¤æ‚ä»»åŠ¡",
                # "complex_tasks": "State-based - é€‚åˆéœ€è¦æ™ºèƒ½å†³ç­–çš„å¤æ‚ä»»åŠ¡",
            },
        }

    def print_detailed_report(self):
        """æ‰“å°è¯¦ç»†æŠ¥å‘Š"""
        report = self.generate_comparison_report()

        print("\n" + "=" * 60)
        print("ğŸ“Š æ ‡å‡†åŒ–æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print("=" * 60)

        # æ€»ä½“æ’å
        print(f"\nğŸ† ç»¼åˆæ’å:")
        for i, item in enumerate(report["ranking"], 1):
            print(f"{i}. {item['pattern']}: {item['score']:.1f}åˆ†")

        # è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
        print(f"\nğŸ“ˆ è¯¦ç»†æ€§èƒ½æŒ‡æ ‡:")
        for pattern, stats in report["pattern_performance"].items():
            print(f"\n{pattern}:")
            print(f"  âœ… æˆåŠŸç‡: {stats['success_rate']:.1%}")
            print(f"  â±ï¸  å¹³å‡å»¶è¿Ÿ: {stats['avg_latency']:.2f}s")
            print(f"  ğŸ“ å¹³å‡å“åº”é•¿åº¦: {stats['avg_response_length']:.0f} å­—ç¬¦")
            print(f"  ğŸ”§ æ€»å·¥å…·è°ƒç”¨: {stats['total_tool_calls']} æ¬¡")

        # å¤æ‚åº¦åˆ†æ
        print(f"\nğŸ¯ æŒ‰å¤æ‚åº¦å¯¹æ¯”:")
        for complexity, analysis in report["complexity_analysis"].items():
            print(f"\n{complexity.upper()}:")
            for pattern, latency in analysis["avg_latency_by_pattern"].items():
                print(f"  {pattern}: {latency:.2f}s")

        # ä½¿ç”¨å»ºè®®
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        for task_type, recommendation in report["summary"]["recommendations"].items():
            print(f"  {task_type}: {recommendation}")

    def save_report(self, filename: str = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report = self.generate_comparison_report()

        if filename is None:
            filename = f"standardized_test_report_{int(time.time())}.json"

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {filename}")
        return filename


# ä¾¿åˆ©å‡½æ•°ï¼šå¿«é€Ÿæµ‹è¯•
async def quick_comparison_test():
    """å¿«é€Ÿå¯¹æ¯”æµ‹è¯• - ä»…ç”¨äºæ¼”ç¤º"""
    from src.agent.pattern_react import graph_pattern_react
    from src.agent.pattern_sequential import graph_pattern_sequential

    # from src.agent.pattern_stateful import graph_pattern_stateful

    patterns = {
        "ReAct": graph_pattern_react,
        "Sequential": graph_pattern_sequential,
        # "State-based": graph_pattern_stateful
    }

    tester = StandardizedTester()
    await tester.run_comprehensive_test(patterns, queries_per_complexity=1)
    tester.print_detailed_report()
    return tester.save_report()


if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    print("æ ‡å‡†åŒ–æµ‹è¯•æ¡†æ¶å·²å‡†å¤‡å°±ç»ª")
    print("ä½¿ç”¨ç¤ºä¾‹:")
    print(
        "  python -c 'import asyncio; from src.evaluation.standardized_tests import quick_comparison_test; asyncio.run(quick_comparison_test())'"
    )
